{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import _pickle as pickle\n",
    "import numpy as np\n",
    "import jieba\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from sklearn import metrics\n",
    "import collections\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def save_variable(v,filename):\n",
    "    f=open(filename,'wb')\n",
    "    pickle.dump(v,f)\n",
    "    f.close()\n",
    "    return filename\n",
    " \n",
    "def load_variavle(filename):\n",
    "    f=open(filename,'rb')\n",
    "    r=pickle.load(f)\n",
    "    f.close()\n",
    "    return r\n",
    "\n",
    "def word2idx_forward(doc, vocab, MAX_SENT_LENGTH, MAX_SENTS):\n",
    "    sents = []\n",
    "    for sent in doc:\n",
    "        sent = [vocab[word] for word in jieba.lcut(sent) if word in vocab]\n",
    "        sents.append(sent[:MAX_SENT_LENGTH] + [0] * (MAX_SENT_LENGTH - len(sent)))\n",
    "    sents = np.array(sents[:MAX_SENTS])\n",
    "    if len(sents) == MAX_SENTS:\n",
    "        return sents\n",
    "    else:\n",
    "        return np.r_[sents, np.array([[0] * MAX_SENT_LENGTH] * (MAX_SENTS - len(sents)))]\n",
    "\n",
    "\n",
    "def word2idx_backward(doc, vocab, MAX_SENT_LENGTH, MAX_SENTS):\n",
    "    sents = []\n",
    "    for sent in doc[::-1]:\n",
    "        sent = [vocab[word] for word in jieba.lcut(sent) if word in vocab]\n",
    "        sents.append(sent[:MAX_SENT_LENGTH] + [0] * (MAX_SENT_LENGTH - len(sent)))\n",
    "    sents = np.array(sents[:MAX_SENTS])\n",
    "    if len(sents) == MAX_SENTS:\n",
    "        return sents[::-1]\n",
    "    else:\n",
    "        return np.r_[sents[::-1], np.array([[0] * MAX_SENT_LENGTH] * (MAX_SENTS - len(sents)))]\n",
    "\n",
    "def sequence_padding(inputs, length=None, padding=0):\n",
    "    \"\"\"Numpy函数，将序列padding到同一长度\n",
    "    inputs: [bs,senctences,sentence_len]\n",
    "    \"\"\"\n",
    "    if length is None:\n",
    "        length = max([len(x) for x in inputs]) \n",
    "    pad_width = [(0, 0) for _ in np.shape(inputs[0])] \n",
    "    outputs = []\n",
    "    for x in inputs:\n",
    "        x = x[:length]\n",
    "        pad_width[0] = (0, length - len(x))\n",
    "        x = np.pad(x, pad_width, 'constant', constant_values=padding)\n",
    "        outputs.append(x)\n",
    "\n",
    "    return np.array(outputs)\n",
    "\n",
    "\n",
    "class data_generator_forward(object):\n",
    "    \"\"\"数据生成器\n",
    "    \"\"\"\n",
    "    def __init__(self, data, batch_size, vocab, MAX_SENT_LENGTH, MAX_SENTS, train = True,buffer_size = None):\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "        self.train = train\n",
    "        self.vocab = vocab\n",
    "        self.MAX_SENT_LENGTH = MAX_SENT_LENGTH\n",
    "        self.MAX_SENTS = MAX_SENTS\n",
    "        if hasattr(self.data, '__len__'):\n",
    "            self.steps = len(self.data) // self.batch_size # 向下取整\n",
    "            if len(self.data) % self.batch_size != 0:\n",
    "                self.steps += 1\n",
    "        else:\n",
    "            self.steps = None\n",
    "        self.buffer_size = buffer_size or batch_size * 1000\n",
    "    def __len__(self):\n",
    "        return self.steps\n",
    "    \n",
    "    def sample(self, random=False):\n",
    "        if random:\n",
    "            if self.steps is None:\n",
    "                def generator():\n",
    "                    caches, isfull = [], False\n",
    "                    for d in self.data:\n",
    "                        caches.append(d)\n",
    "                        if isfull:\n",
    "                            i = np.random.randint(len(caches))\n",
    "                            yield chaches.pop(i)\n",
    "                        elif len(caches) == self.buffer_size:\n",
    "                            isfull = True\n",
    "                        while caches:\n",
    "                            i = np.random.randint(len(caches))\n",
    "                            yield caches.pop(i)\n",
    "            else:\n",
    "                def generator():\n",
    "                    indices = list(range(len(self.data)))\n",
    "                    np.random.shuffle(indices)\n",
    "                    for i in indices:\n",
    "                        yield self.data[i]\n",
    "            data = generator()\n",
    "        else:\n",
    "            data = iter(self.data)\n",
    "        \n",
    "        d_current = next(data) \n",
    "        for d_next in data:\n",
    "            yield False, d_current\n",
    "            d_current = d_next\n",
    "        \n",
    "        yield True, d_current\n",
    "        \n",
    "    def __iter__(self, random=False):\n",
    "        batch_token_ids, batch_segment_ids, batch_labels = [], [], []\n",
    "        if self.train == True:\n",
    "            for is_end, (ids, text, label) in self.sample(random):\n",
    "                batch_token_ids.append(word2idx_forward(text, self.vocab, self.MAX_SENT_LENGTH, self.MAX_SENTS))\n",
    "                batch_labels.append([label])\n",
    "                if len(batch_token_ids) == self.batch_size or is_end:\n",
    "                    batch_token_ids = sequence_padding(batch_token_ids)\n",
    "                    batch_labels = sequence_padding(batch_labels)\n",
    "                    yield batch_token_ids, batch_labels\n",
    "                    batch_token_ids, batch_segment_ids, batch_labels = [], [], []\n",
    "        else:\n",
    "            for is_end, (ids, text, label) in self.sample(random):\n",
    "                batch_token_ids.append(word2idx_forward(text, self.vocab, self.MAX_SENT_LENGTH, self.MAX_SENTS))\n",
    "                if len(batch_token_ids) == self.batch_size or is_end:\n",
    "                    batch_token_ids = sequence_padding(batch_token_ids)\n",
    "                    yield batch_token_ids\n",
    "                    batch_token_ids = []\n",
    "        \n",
    "    def forfit(self, random=True):\n",
    "        while True:\n",
    "            for d in self.__iter__(random):\n",
    "                yield d\n",
    "    \n",
    "    def forpredict(self, random=False):\n",
    "        while True:\n",
    "            for d in self.__iter__(random):\n",
    "                yield d\n",
    "\n",
    "class data_generator_backward(object):\n",
    "    \"\"\"数据生成器\n",
    "    \"\"\"\n",
    "    def __init__(self, data, batch_size,vocab, MAX_SENT_LENGTH, MAX_SENTS, train = True,buffer_size = None):\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "        self.train = train\n",
    "        self.vocab = vocab\n",
    "        self.MAX_SENT_LENGTH = MAX_SENT_LENGTH\n",
    "        self.MAX_SENTS = MAX_SENTS\n",
    "        if hasattr(self.data, '__len__'):\n",
    "            self.steps = len(self.data) // self.batch_size\n",
    "            if len(self.data) % self.batch_size != 0:\n",
    "                self.steps += 1\n",
    "        else:\n",
    "            self.steps = None\n",
    "        self.buffer_size = buffer_size or batch_size * 1000\n",
    "    def __len__(self):\n",
    "        return self.steps\n",
    "    \n",
    "    def sample(self, random=False):\n",
    "        if random:\n",
    "            if self.steps is None:\n",
    "                def generator():\n",
    "                    caches, isfull = [], False\n",
    "                    for d in self.data:\n",
    "                        caches.append(d)\n",
    "                        if isfull:\n",
    "                            i = np.random.randint(len(caches))\n",
    "                            yield chaches.pop(i)\n",
    "                        elif len(caches) == self.buffer_size:\n",
    "                            isfull = True\n",
    "                        while caches:\n",
    "                            i = np.random.randint(len(caches))\n",
    "                            yield caches.pop(i)\n",
    "            else:\n",
    "                def generator():\n",
    "                    indices = list(range(len(self.data)))\n",
    "                    np.random.shuffle(indices)\n",
    "                    for i in indices:\n",
    "                        yield self.data[i]\n",
    "            data = generator()\n",
    "        else:\n",
    "            data = iter(self.data)\n",
    "        \n",
    "        d_current = next(data)\n",
    "        for d_next in data:\n",
    "            yield False, d_current\n",
    "            d_current = d_next\n",
    "        \n",
    "        yield True, d_current\n",
    "        \n",
    "    def __iter__(self, random=False):\n",
    "        batch_token_ids, batch_segment_ids, batch_labels = [], [], []\n",
    "        if self.train == True:\n",
    "            for is_end, (ids, label,text) in self.sample(random):\n",
    "                batch_token_ids.append(word2idx_backward(text, self.vocab, self.MAX_SENT_LENGTH, self.MAX_SENTS))\n",
    "                batch_labels.append([label])\n",
    "                if len(batch_token_ids) == self.batch_size or is_end:\n",
    "                    batch_token_ids = sequence_padding(batch_token_ids)\n",
    "                    batch_labels = sequence_padding(batch_labels)\n",
    "                    yield batch_token_ids, batch_labels\n",
    "                    batch_token_ids, batch_segment_ids, batch_labels = [], [], []\n",
    "        else:\n",
    "            for is_end, (ids, text, label) in self.sample(random):\n",
    "                batch_token_ids.append(word2idx_backward(text, self.vocab, self.MAX_SENT_LENGTH, self.MAX_SENTS))\n",
    "                if len(batch_token_ids) == self.batch_size or is_end:\n",
    "                    batch_token_ids = sequence_padding(batch_token_ids)\n",
    "                    yield batch_token_ids\n",
    "                    batch_token_ids = []\n",
    "        \n",
    "    def forfit(self, random=True):\n",
    "        while True:\n",
    "            for d in self.__iter__(random):\n",
    "                yield d\n",
    "    \n",
    "    def forpredict(self, random=False):\n",
    "        while True:\n",
    "            for d in self.__iter__(random):\n",
    "                yield d\n",
    "def focal_loss(gamma=2., alpha=.25):\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "        return -K.mean(alpha * K.pow(1. - pt_1, gamma) * K.log(K.epsilon()+pt_1))-K.sum((1-alpha) * K.pow( pt_0, gamma) * K.log(1. - pt_0 + K.epsilon()))\n",
    "    return focal_loss_fixed\n",
    "\n",
    "# cut words\n",
    "def cut_text(sentence):\n",
    "    tokens = lac.run(sentence)\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: `shm_size_mb` is a deprecated argument. It will be removed in `pandarallel 2.0.0`.\n",
      "INFO: Pandarallel will run on 24 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n",
      "[['LAC', '是', '个', '优秀', '的', '分词', '工具'], ['nz', 'v', 'q', 'a', 'u', 'n', 'n']]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import gc\n",
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize(5)\n",
    "gc.collect()\n",
    "\n",
    "from LAC import LAC\n",
    "lac = LAC(mode='lac')\n",
    "lac.add_word('【科技实体】')\n",
    "text = u\"LAC是个优秀的分词工具\"\n",
    "lac_result = lac.run(text)\n",
    "print(lac_result)\n",
    "\n",
    "\n",
    "train = pd.read_csv('datasets/Train.csv')\n",
    "test = pd.read_csv('datasets/Test_B.csv')\n",
    "\n",
    "\n",
    "with open('lac/entity_vocab.txt','r' ) as f:\n",
    "    entitys = f.readlines()\n",
    "    entitys = [x.strip() for x in entitys]\n",
    "\n",
    "import json\n",
    "def entityResolution(line):\n",
    "    line = lac.run(line)\n",
    "    words,postag = line[0],line[1]\n",
    "    new_str = ''\n",
    "    for w in words:\n",
    "        if w in entitys:\n",
    "            new_str += '【科技实体】'\n",
    "        else:\n",
    "            new_str += w\n",
    "    return new_str\n",
    "\n",
    "\n",
    "train['Text_new'] = train['Text'].parallel_apply(lambda x: entityResolution(x[:512])) \n",
    "test['Text_new'] = test['Text'].parallel_apply(lambda x: entityResolution(x[:512]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lzy/anaconda3/envs/tensorflow/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "def reposition(data):\n",
    "    mid = data['text']\n",
    "    data.drop(labels=['text'], axis=1,inplace = True)\n",
    "    data.insert(1, 'text', mid)\n",
    "    return data\n",
    "train.drop(columns=['Domain','Abstract','Text'],inplace=True)\n",
    "train.rename(columns={'ID':'id','Text_new':'text','Label':'label'},inplace=True)\n",
    "train = reposition(train)\n",
    "\n",
    "test.drop(columns=['Text'],inplace=True)\n",
    "test.rename(columns={'ID':'id','Text_new':'text'},inplace=True)\n",
    "test = reposition(test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pretrainW2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lzy/anaconda3/envs/tensorflow/lib/python3.7/site-packages/scipy/sparse/sparsetools.py:21: DeprecationWarning: `scipy.sparse.sparsetools` is deprecated!\n",
      "scipy.sparse.sparsetools is a private module for scipy.sparse, and should not be used.\n",
      "  _deprecated()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baca6c97aae844bd8e6abcddf8943381",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='word count ing', max=46940.0, style=ProgressStyle(descrip…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5570bd4e5ed4880b6595217981d8897",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='filt ing...', max=321384.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.models.word2vec:collecting all words and their counts\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始训练词向量\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #10000, processed 2380890 words, keeping 12555 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #20000, processed 4725032 words, keeping 13001 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #30000, processed 6947796 words, keeping 13130 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #40000, processed 9165715 words, keeping 13130 word types\n",
      "INFO:gensim.models.word2vec:collected 13130 word types from a corpus of 10730866 raw words and 46940 sentences\n",
      "INFO:gensim.models.word2vec:Loading a fresh vocabulary\n",
      "INFO:gensim.models.word2vec:effective_min_count=50 retains 13130 unique words (100% of original 13130, drops 0)\n",
      "INFO:gensim.models.word2vec:effective_min_count=50 leaves 10730866 word corpus (100% of original 10730866, drops 0)\n",
      "INFO:gensim.models.word2vec:deleting the raw counts dictionary of 13130 items\n",
      "INFO:gensim.models.word2vec:sample=0.001 downsamples 26 most-common words\n",
      "INFO:gensim.models.word2vec:downsampling leaves estimated 8651136 word corpus (80.6% of prior 10730866)\n",
      "INFO:gensim.models.base_any2vec:estimated required memory for 13130 words and 100 dimensions: 17069000 bytes\n",
      "INFO:gensim.models.word2vec:resetting layer weights\n",
      "/home/lzy/anaconda3/envs/tensorflow/lib/python3.7/site-packages/ipykernel_launcher.py:42: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "INFO:gensim.models.base_any2vec:training model with 5 workers on 13130 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "INFO:gensim.models.base_any2vec:EPOCH 1 - PROGRESS: at 28.49% examples, 2543601 words/s, in_qsize 9, out_qsize 0\n",
      "INFO:gensim.models.base_any2vec:EPOCH 1 - PROGRESS: at 58.50% examples, 2560800 words/s, in_qsize 9, out_qsize 0\n",
      "INFO:gensim.models.base_any2vec:EPOCH 1 - PROGRESS: at 90.53% examples, 2604870 words/s, in_qsize 9, out_qsize 0\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.base_any2vec:EPOCH - 1 : training on 10730866 raw words (8650976 effective words) took 3.3s, 2619480 effective words/s\n",
      "INFO:gensim.models.base_any2vec:EPOCH 2 - PROGRESS: at 24.20% examples, 2165438 words/s, in_qsize 10, out_qsize 0\n",
      "INFO:gensim.models.base_any2vec:EPOCH 2 - PROGRESS: at 53.20% examples, 2335344 words/s, in_qsize 9, out_qsize 0\n",
      "INFO:gensim.models.base_any2vec:EPOCH 2 - PROGRESS: at 84.05% examples, 2424656 words/s, in_qsize 9, out_qsize 0\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.base_any2vec:EPOCH - 2 : training on 10730866 raw words (8650631 effective words) took 3.5s, 2438204 effective words/s\n",
      "INFO:gensim.models.base_any2vec:EPOCH 3 - PROGRESS: at 29.56% examples, 2642066 words/s, in_qsize 9, out_qsize 0\n",
      "INFO:gensim.models.base_any2vec:EPOCH 3 - PROGRESS: at 59.55% examples, 2604492 words/s, in_qsize 9, out_qsize 0\n",
      "INFO:gensim.models.base_any2vec:EPOCH 3 - PROGRESS: at 87.77% examples, 2527700 words/s, in_qsize 9, out_qsize 0\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.base_any2vec:EPOCH - 3 : training on 10730866 raw words (8651105 effective words) took 3.5s, 2505375 effective words/s\n",
      "INFO:gensim.models.base_any2vec:EPOCH 4 - PROGRESS: at 27.32% examples, 2443582 words/s, in_qsize 9, out_qsize 0\n",
      "INFO:gensim.models.base_any2vec:EPOCH 4 - PROGRESS: at 55.75% examples, 2444090 words/s, in_qsize 9, out_qsize 0\n",
      "INFO:gensim.models.base_any2vec:EPOCH 4 - PROGRESS: at 88.94% examples, 2561162 words/s, in_qsize 9, out_qsize 0\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.base_any2vec:EPOCH - 4 : training on 10730866 raw words (8650434 effective words) took 3.4s, 2524779 effective words/s\n",
      "INFO:gensim.models.base_any2vec:EPOCH 5 - PROGRESS: at 29.65% examples, 2640540 words/s, in_qsize 9, out_qsize 0\n",
      "INFO:gensim.models.base_any2vec:EPOCH 5 - PROGRESS: at 59.06% examples, 2581104 words/s, in_qsize 9, out_qsize 0\n",
      "INFO:gensim.models.base_any2vec:EPOCH 5 - PROGRESS: at 88.38% examples, 2542298 words/s, in_qsize 9, out_qsize 0\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.base_any2vec:EPOCH - 5 : training on 10730866 raw words (8651282 effective words) took 3.4s, 2521550 effective words/s\n",
      "INFO:gensim.models.base_any2vec:EPOCH 6 - PROGRESS: at 25.81% examples, 2307924 words/s, in_qsize 10, out_qsize 0\n",
      "INFO:gensim.models.base_any2vec:EPOCH 6 - PROGRESS: at 52.72% examples, 2314943 words/s, in_qsize 9, out_qsize 0\n",
      "INFO:gensim.models.base_any2vec:EPOCH 6 - PROGRESS: at 79.61% examples, 2297016 words/s, in_qsize 9, out_qsize 0\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.base_any2vec:EPOCH - 6 : training on 10730866 raw words (8650477 effective words) took 3.7s, 2312135 effective words/s\n",
      "INFO:gensim.models.base_any2vec:EPOCH 7 - PROGRESS: at 29.29% examples, 2616986 words/s, in_qsize 9, out_qsize 0\n",
      "INFO:gensim.models.base_any2vec:EPOCH 7 - PROGRESS: at 60.02% examples, 2620674 words/s, in_qsize 9, out_qsize 0\n",
      "INFO:gensim.models.base_any2vec:EPOCH 7 - PROGRESS: at 91.82% examples, 2641116 words/s, in_qsize 9, out_qsize 0\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.base_any2vec:EPOCH - 7 : training on 10730866 raw words (8651629 effective words) took 3.3s, 2637726 effective words/s\n",
      "INFO:gensim.models.base_any2vec:EPOCH 8 - PROGRESS: at 27.68% examples, 2473977 words/s, in_qsize 9, out_qsize 0\n",
      "INFO:gensim.models.base_any2vec:EPOCH 8 - PROGRESS: at 59.55% examples, 2603576 words/s, in_qsize 9, out_qsize 0\n",
      "INFO:gensim.models.base_any2vec:EPOCH 8 - PROGRESS: at 90.71% examples, 2610498 words/s, in_qsize 9, out_qsize 0\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.base_any2vec:EPOCH - 8 : training on 10730866 raw words (8650513 effective words) took 3.3s, 2626287 effective words/s\n",
      "INFO:gensim.models.base_any2vec:training on a 85846928 raw words (69207047 effective words) took 27.5s, 2517508 effective words/s\n",
      "INFO:gensim.utils:saving Word2Vec object under model_han_fin/w2v.model, separately None\n",
      "INFO:gensim.utils:not storing attribute vectors_norm\n",
      "INFO:gensim.utils:not storing attribute cum_table\n",
      "INFO:gensim.utils:saved model_han_fin/w2v.model\n",
      "INFO:gensim.models.utils_any2vec:storing 13130x100 projection weights into model_han_fin/word2vec_model.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import jieba\n",
    "import gensim\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "train = train\n",
    "test = test\n",
    "\n",
    "train_labels = train.label.values\n",
    "\n",
    "stop_words = []\n",
    "\n",
    "vocab_set_path = 'model_han_fin/vocab_set.pkl'\n",
    "test['label'] = 0\n",
    "\n",
    "train['text'] = train['text'].parallel_apply(lambda x: cut_text(x)[0])\n",
    "test['text'] = test['text'].parallel_apply(lambda x: cut_text(x)[0])\n",
    "#词语计数\n",
    "\n",
    "word_cnt = collections.Counter()\n",
    "for line in tqdm(np.r_[train['text'].values,test['text'].values],desc= 'word count ing'):\n",
    "    word_cnt.update(line)\n",
    "min_count = 50\n",
    "word_cnt = [k for k in tqdm(word_cnt,desc='filt ing...') if word_cnt[k] > min_count] \n",
    "word_cnt = {k:v for v,k in enumerate(word_cnt)}\n",
    "\n",
    "vocab = word_cnt\n",
    "\n",
    "train['text'] = train['text'].parallel_apply(lambda x:[i for i in x if i in vocab])\n",
    "test['text'] = test['text'].parallel_apply(lambda x:[i for i in x if i in vocab])\n",
    "\n",
    "# save vocab\n",
    "save_variable(vocab,vocab_set_path) \n",
    "\n",
    "print('开始训练词向量')\n",
    "vector_size = 100 #2\n",
    "model = gensim.models.Word2Vec(size=vector_size, window=5, min_count=50, workers=5, sg=0, iter=8, seed=2021)\n",
    "model.build_vocab(np.r_[train['text'].values,test['text'].values])\n",
    "model.train(np.r_[train['text'].values,test['text'].values], total_examples=model.corpus_count, epochs=model.iter)\n",
    "model.save(\"model_han_fin/w2v.model\")\n",
    "word2vec_save = 'model_han_fin/word2vec_model.txt'\n",
    "model.wv.save_word2vec_format(word2vec_save, binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lzy/anaconda3/envs/tensorflow/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "from keras.models import Model\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from bert4keras.backend import keras, set_gelu\n",
    "from keras.layers import *\n",
    "from keras import backend as K\n",
    "from bert4keras.optimizers import Adam, extend_with_piecewise_linear_lr\n",
    "\n",
    "class AttentionLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__( **kwargs)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "        self.W = self.add_weight(name='att_weight',\n",
    "                                shape=(input_shape[1], input_shape[1]),\n",
    "                                initializer='uniform',\n",
    "                                trainable=True)\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "    \n",
    "    def call(self, inputs, mask=None):\n",
    "        x = K.permute_dimensions(inputs, (0, 2, 1))\n",
    "        a = K.softmax(K.tanh(K.dot(x, self.W)))\n",
    "        a = K.permute_dimensions(a, (0, 2, 1))\n",
    "        outputs = a * inputs\n",
    "        outputs = K.sum(outputs, axis=1)\n",
    "        return outputs\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], input_shape[2]\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "class SetLearningRate:\n",
    "    \"\"\"\n",
    "    \tlayer learning rate\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, layer, lamb, is_ada=False):\n",
    "        self.layer = layer\n",
    "        self.lamb = lamb # learning rate\n",
    "        self.is_ada = is_ada # if adam\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        with K.name_scope(self.layer.name):\n",
    "            if not self.layer.built:\n",
    "                input_shape = K.int_shape(inputs)\n",
    "                self.layer.build(input_shape)\n",
    "                self.layer.built = True\n",
    "                if self.layer._initial_weights is not None:\n",
    "                    self.layer.set_weights(self.layer._initial_weights)\n",
    "        for key in ['kernel', 'bias', 'embed', 'depthwise_kernel', 'pointwise_kernel', 'recurrent_kernel', 'gamma', 'beta']:\n",
    "            if hasattr(self.layer, key):\n",
    "                weight = getattr(self.layer, key)\n",
    "                if self.is_ada:\n",
    "                    lamb = self.lamb \n",
    "                else:\n",
    "                    lamb = self.lamb**0.5 \n",
    "                K.set_value(weight, K.eval(weight) / lamb) \n",
    "                setattr(self.layer, key, weight * lamb) \n",
    "        return self.layer(inputs)\n",
    "\n",
    "\n",
    "class my_HAN_model(object):\n",
    "    def __init__(self, MAX_SENT_LENGTH, MAX_SENTS, embedding_matrix, vocab):\n",
    "    \tself.MAX_SENT_LENGTH = MAX_SENT_LENGTH\n",
    "    \tself.MAX_SENTS = MAX_SENTS\n",
    "    \tself.embedding_matrix = embedding_matrix\n",
    "    \tself.vocab = vocab\n",
    "    def create_model(self):\n",
    "        sentence_inputs = Input(shape=(self.MAX_SENT_LENGTH,), dtype='float64')\n",
    "        \n",
    "        embed = Embedding(len(self.vocab) + 1, 100, input_length=self.MAX_SENT_LENGTH, weights=[self.embedding_matrix], trainable=True)\n",
    "        embed = SetLearningRate(embed, 0.001, True)(sentence_inputs)\n",
    "        l_lstm = Bidirectional(GRU(128, return_sequences=True))(embed)\n",
    "        l_dense = TimeDistributed(Dense(64))(l_lstm)\n",
    "        print(l_dense.shape)\n",
    "        l_att = AttentionLayer()(l_dense)\n",
    "        print(l_att.shape)\n",
    "        sentEncoder = Model(sentence_inputs, l_att)\n",
    "        \n",
    "        review_input = Input(shape=(self.MAX_SENTS, self.MAX_SENT_LENGTH), dtype='int32')\n",
    "        review_encoder = TimeDistributed(sentEncoder)(review_input)\n",
    "        l_lstm_sent = Bidirectional(GRU(128, return_sequences=True))(review_encoder)\n",
    "        l_dense_sent = TimeDistributed(Dense(64))(l_lstm_sent)\n",
    "        l_att_sent = AttentionLayer()(l_dense_sent)\n",
    "        outputs = Dense(1, activation='sigmoid')(l_att_sent)\n",
    "        \n",
    "        self.model = Model(review_input, outputs=outputs)\n",
    "        self.compile()\n",
    "        return self.model\n",
    "    \n",
    "    def compile(self):\n",
    "        self.model.compile(\n",
    "             loss='binary_crossentropy',\n",
    "            optimizer=Adam(1e-3),  \n",
    "            metrics=['accuracy'],\n",
    "        )\n",
    "\n",
    "epsilon = 1e-5\n",
    "smooth = 1\n",
    "def tversky(y_true, y_pred):\n",
    "    y_true_pos = K.flatten(y_true)\n",
    "    y_pred_pos = K.flatten(y_pred)\n",
    "    true_pos = K.sum(y_true_pos * y_pred_pos)\n",
    "    false_neg = K.sum(y_true_pos * (1-y_pred_pos))\n",
    "    false_pos = K.sum((1-y_true_pos)*y_pred_pos)\n",
    "    alpha = 0.7\n",
    "    return (true_pos + smooth)/(true_pos + alpha*false_neg + (1-alpha)*false_pos + smooth)\n",
    "\n",
    "def tversky_loss(y_true, y_pred):\n",
    "    return 1 - tversky(y_true, y_pred)\n",
    "\n",
    "def focal_tversky(y_true,y_pred):\n",
    "    pt_1 = tversky(y_true, y_pred)\n",
    "    gamma = 0.75\n",
    "    return K.pow((1-pt_1), gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 4 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize(nb_workers = 4)\n",
    "def mysplit_keep_puntuation(s, punctuation='!~。？?,，；'):\n",
    "    res_list = []\n",
    "    buff = ''\n",
    "\n",
    "    for c in s:\n",
    "        buff += c\n",
    "        if c in punctuation:\n",
    "            res_list.append(buff)\n",
    "            buff = ''\n",
    "    if buff != '':\n",
    "        res_list.append(buff)\n",
    "    return res_list\n",
    "def reposition(data):\n",
    "    mid = data['text']\n",
    "    data.drop(labels=['text'], axis=1,inplace = True)\n",
    "    data.insert(1, 'text', mid)\n",
    "    return data\n",
    "def dataProcess(data,vocab):\n",
    "    data['text'] = data['text'].parallel_apply(lambda x: cut_text(x)[0])\n",
    "    data['text'] = data['text'].parallel_apply(lambda x:''.join([i for i in x if i in vocab]))\n",
    "    data['text'] = data['text'].parallel_apply(lambda x:mysplit_keep_puntuation(x,'。'))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.utils:loading Word2Vec object from model_han_fin/w2v.model\n",
      "INFO:gensim.utils:loading wv recursively from model_han_fin/w2v.model.wv.* with mmap=None\n",
      "INFO:gensim.utils:setting ignored attribute vectors_norm to None\n",
      "INFO:gensim.utils:loading vocabulary recursively from model_han_fin/w2v.model.vocabulary.* with mmap=None\n",
      "INFO:gensim.utils:loading trainables recursively from model_han_fin/w2v.model.trainables.* with mmap=None\n",
      "INFO:gensim.utils:setting ignored attribute cum_table to None\n",
      "INFO:gensim.utils:loaded model_han_fin/w2v.model\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Model\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from bert4keras.backend import keras, set_gelu\n",
    "from keras.layers import *\n",
    "from keras import backend as K\n",
    "from sklearn import metrics\n",
    "import jieba\n",
    "import gensim\n",
    "import os\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import Sequence\n",
    "from bert4keras.optimizers import Adam, extend_with_piecewise_linear_lr\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "resample_flag = True\n",
    "MAX_SENT_LENGTH = 100\n",
    "MAX_SENTS = 10\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "vocab = load_variavle('model_han_fin/vocab_set.pkl')\n",
    "w2v_model = gensim.models.Word2Vec.load(\"model_han_fin/w2v.model\")\n",
    "\n",
    "train = pd.read_csv(\"datasets/Train.csv\")\n",
    "train['Text_new'] = train['Text'].parallel_apply(lambda x: entityResolution(x[:512]))\n",
    "train.drop(columns=['Domain','Abstract','Text'],inplace=True)\n",
    "train.rename(columns={'ID':'id','Text_new':'text','Label':'label'},inplace=True)\n",
    "train = reposition(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lzy/anaconda3/envs/tensorflow/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "def evaluate(data,model):\n",
    "    total, right = 0., 0.\n",
    "    y_trues = []\n",
    "    y_preds = []\n",
    "    for x_true, y_true in data:\n",
    "        y_preds = np.r_[y_preds, model.predict(x_true)[:,0]]\n",
    "        y_trues = np.r_[y_trues, y_true[:, 0]]\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_trues, y_preds)\n",
    "    return  metrics.auc(fpr, tpr)\n",
    "\n",
    "def evaluate_recall(data,model):\n",
    "    total, right = 0., 0.\n",
    "    y_trues = []\n",
    "    y_preds = []\n",
    "    for x_true, y_true in data:\n",
    "        y_preds = np.r_[y_preds, model.predict(x_true)[:,0]]\n",
    "        y_trues = np.r_[y_trues, y_true[:, 0]]\n",
    "    fscore = metrics.f1_score(y_trues, y_preds > 0.5, pos_label=1, average='binary')\n",
    "    \n",
    "    print(fscore)\n",
    "    thresholds = list(np.arange(0.0, 1.0, 0.001))\n",
    "    acc_scores = np.zeros(shape=(len(thresholds)))\n",
    "    for index, elem in tqdm(enumerate(thresholds),total=len(thresholds),disable = True):\n",
    "        y_pred_prob = (y_preds > elem).astype('int')\n",
    "        acc_scores[index] =  metrics.f1_score(y_trues,y_pred_prob,pos_label=1,average='binary')\n",
    "    index = np.argmax(acc_scores)\n",
    "    thresholdOpt = round(thresholds[index], ndigits = 5)\n",
    "    best = round(acc_scores[index], ndigits = 5)\n",
    "    return  best,thresholdOpt\n",
    "class Evaluator(keras.callbacks.Callback):\n",
    "    \"\"\"评估与保存\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name,valid_generator,model):\n",
    "        self.best_val_acc = 0\n",
    "        self.model_name = model_name\n",
    "        self.valid_generator = valid_generator\n",
    "        self.model = model\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        val_acc = evaluate(self.valid_generator, self.model) #auc\n",
    "        best,threshold = evaluate_recall(self.valid_generator, self.model)\n",
    "        if (best > self.best_val_acc):\n",
    "            print('1')\n",
    "            self.best_val_acc = best\n",
    "            self.best_threshold = threshold\n",
    "            self.model.save_weights(self.model_name)\n",
    "        print(\n",
    "            u'val_auc: %.5f, best_val_auc: %.5f, val_f1: %.5f' %\n",
    "            (val_acc, self.best_val_acc, best)\n",
    "        )\n",
    "        logs['val_auc'] = val_acc\n",
    "        logs['val_recall'] = best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lzy/anaconda3/envs/tensorflow/lib/python3.7/site-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  after removing the cwd from sys.path.\n",
      "/home/lzy/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/framework/tensor_util.py:523: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  tensor_proto.tensor_content = nparray.tostring()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 100, 64)\n",
      "(None, 64)\n",
      "开始训练\n",
      "Epoch 1/2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "339/339 [==============================] - 10s 28ms/step - loss: 0.3058 - accuracy: 0.8849\n",
      "Epoch 2/2\n",
      "339/339 [==============================] - 10s 28ms/step - loss: 0.1410 - accuracy: 0.9517\n"
     ]
    }
   ],
   "source": [
    "\n",
    "embedding_matrix = np.zeros((len(vocab) + 1, 100))\n",
    "for word, i in vocab.items():\n",
    "    try:\n",
    "        embedding_vector = w2v_model[str(word)]\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    except KeyError:\n",
    "        continue\n",
    "\n",
    "# 5折\n",
    "train_entity = train[train['text'].str.contains('【科技实体】')]\n",
    "train_pos = train[train['label']==1]\n",
    "train_neg = train[train['label']==0].sample(2000,random_state=123)\n",
    "train_df = pd.concat([train_entity,train_pos,train_neg]).drop_duplicates()\n",
    "train_df = dataProcess(train_df,vocab)\n",
    "\n",
    "train_batch_size = 8 \n",
    "\n",
    "K.clear_session()\n",
    "md = my_HAN_model(MAX_SENT_LENGTH = MAX_SENT_LENGTH, MAX_SENTS=MAX_SENTS, embedding_matrix=embedding_matrix,vocab=vocab)\n",
    "model = md.create_model()\n",
    "train_generator = data_generator_forward(train_df.values, train_batch_size, vocab, MAX_SENT_LENGTH, MAX_SENTS) \n",
    "print(\"开始训练\")\n",
    "model.fit_generator(\n",
    "    train_generator.forfit(), \n",
    "    steps_per_epoch=len(train_generator),\n",
    "    epochs= 2,\n",
    "\n",
    ")\n",
    "model.save_weights('model_han_fin/best_model{}.weights'.format(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('datasets/Test_B.csv')\n",
    "test['Text_new'] = test['Text'].parallel_apply(lambda x: entityResolution(x[:512]))\n",
    "test.drop(columns =['Text'],inplace=True)\n",
    "test.rename(columns={'ID':'id','Text_new':'text'},inplace=True)\n",
    "test = reposition(test)\n",
    "test = dataProcess(test,vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lzy/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/framework/tensor_util.py:523: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  tensor_proto.tensor_content = nparray.tostring()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 100, 64)\n",
      "(None, 64)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "\tUse thie file to generate result.\n",
    "'''\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "from keras.models import Model\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from bert4keras.backend import keras, set_gelu\n",
    "from keras.layers import *\n",
    "from keras import backend as K\n",
    "from sklearn import metrics\n",
    "import gensim\n",
    "import os\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import Sequence\n",
    "from bert4keras.optimizers import Adam, extend_with_piecewise_linear_lr\n",
    "\n",
    "batch_size = 32\n",
    "test['Label'] = 0\n",
    "\n",
    "\n",
    "K.clear_session()\n",
    "md = my_HAN_model(MAX_SENT_LENGTH = MAX_SENT_LENGTH, MAX_SENTS=MAX_SENTS, embedding_matrix=embedding_matrix,vocab=vocab)\n",
    "model = md.create_model()\n",
    "test_generator = data_generator_forward(test.values, batch_size, vocab, MAX_SENT_LENGTH, MAX_SENTS, train=False)\n",
    "model.load_weights(r'model_han_fin/best_model{}.weights'.format(1))\n",
    "probs = model.predict_generator(test_generator.forpredict(),\n",
    "    steps=len(test_generator))[:,0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.array(probs)\n",
    "r = []\n",
    "for pred in y_pred:\n",
    "    pred = np.where(pred > 0.3,1,0)\n",
    "    r.append(pred)\n",
    "r = np.array(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1495</th>\n",
       "      <td>1495</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1496</th>\n",
       "      <td>1496</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1497</th>\n",
       "      <td>1497</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1498</th>\n",
       "      <td>1498</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1499</th>\n",
       "      <td>1499</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1500 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ID  Label\n",
       "0        0      0\n",
       "1        1      0\n",
       "2        2      1\n",
       "3        3      0\n",
       "4        4      1\n",
       "...    ...    ...\n",
       "1495  1495      0\n",
       "1496  1496      0\n",
       "1497  1497      0\n",
       "1498  1498      0\n",
       "1499  1499      0\n",
       "\n",
       "[1500 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test = pd.read_csv('datasets/Test_B.csv')\n",
    "submit_data = test[['ID']]\n",
    "submit_data[\"Label\"]=r\n",
    "submit_data.to_csv(\"result/result_han_0.68.csv\",index=False)\n",
    "display(submit_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lzy/anaconda3/envs/tensorflow/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6862170087976539"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true = [1] * 212 + [0] * 1288\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "f1_score(true,r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
