{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import _pickle as pickle\n",
    "import numpy as np\n",
    "import jieba\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from sklearn import metrics\n",
    "import collections\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def save_variable(v,filename):\n",
    "    f=open(filename,'wb')\n",
    "    pickle.dump(v,f)#序列化对象，将对象obj保存到文件file中去\n",
    "    f.close()\n",
    "    return filename\n",
    " \n",
    "def load_variavle(filename):\n",
    "    f=open(filename,'rb')\n",
    "    r=pickle.load(f)\n",
    "    f.close()\n",
    "    return r\n",
    "\n",
    "def word2idx_forward(doc, vocab, MAX_SENT_LENGTH, MAX_SENTS):\n",
    "    sents = []\n",
    "    for sent in doc:\n",
    "        sent = [vocab[word] for word in jieba.lcut(sent) if word in vocab]\n",
    "        sents.append(sent[:MAX_SENT_LENGTH] + [0] * (MAX_SENT_LENGTH - len(sent)))\n",
    "    sents = np.array(sents[:MAX_SENTS])\n",
    "    if len(sents) == MAX_SENTS:\n",
    "        return sents\n",
    "    else:\n",
    "        return np.r_[sents, np.array([[0] * MAX_SENT_LENGTH] * (MAX_SENTS - len(sents)))]\n",
    "\n",
    "# last 350 sentences\n",
    "def word2idx_backward(doc, vocab, MAX_SENT_LENGTH, MAX_SENTS):\n",
    "    sents = []\n",
    "    for sent in doc[::-1]:\n",
    "        sent = [vocab[word] for word in jieba.lcut(sent) if word in vocab]\n",
    "        sents.append(sent[:MAX_SENT_LENGTH] + [0] * (MAX_SENT_LENGTH - len(sent)))\n",
    "    sents = np.array(sents[:MAX_SENTS])\n",
    "    if len(sents) == MAX_SENTS:\n",
    "        return sents[::-1]\n",
    "    else:\n",
    "        return np.r_[sents[::-1], np.array([[0] * MAX_SENT_LENGTH] * (MAX_SENTS - len(sents)))]\n",
    "\n",
    "def sequence_padding(inputs, length=None, padding=0):\n",
    "    \"\"\"Numpy函数，将序列padding到同一长度\n",
    "    inputs: [bs,senctences,sentence_len]\n",
    "    \"\"\"\n",
    "    if length is None:\n",
    "        length = max([len(x) for x in inputs]) #length : 文档最大句子数\n",
    "    pad_width = [(0, 0) for _ in np.shape(inputs[0])] # [(0,0),(0,0)]\n",
    "    outputs = []\n",
    "    for x in inputs:\n",
    "        x = x[:length]\n",
    "        pad_width[0] = (0, length - len(x)) # [(0,pad_length),(0,0)]\n",
    "        x = np.pad(x, pad_width, 'constant', constant_values=padding)\n",
    "        outputs.append(x)\n",
    "\n",
    "    return np.array(outputs)\n",
    "\n",
    "\n",
    "class data_generator_forward(object):\n",
    "    \"\"\"数据生成器\n",
    "    \"\"\"\n",
    "    def __init__(self, data, batch_size, vocab, MAX_SENT_LENGTH, MAX_SENTS, train = True,buffer_size = None):\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "        self.train = train\n",
    "        self.vocab = vocab\n",
    "        self.MAX_SENT_LENGTH = MAX_SENT_LENGTH\n",
    "        self.MAX_SENTS = MAX_SENTS\n",
    "        if hasattr(self.data, '__len__'):\n",
    "            self.steps = len(self.data) // self.batch_size # 向下取整\n",
    "            if len(self.data) % self.batch_size != 0:\n",
    "                self.steps += 1\n",
    "        else:\n",
    "            self.steps = None\n",
    "        self.buffer_size = buffer_size or batch_size * 1000\n",
    "    def __len__(self):\n",
    "        return self.steps\n",
    "    \n",
    "    def sample(self, random=False):\n",
    "        if random:\n",
    "            if self.steps is None:\n",
    "                def generator():\n",
    "                    caches, isfull = [], False\n",
    "                    for d in self.data:\n",
    "                        caches.append(d)\n",
    "                        if isfull:\n",
    "                            i = np.random.randint(len(caches))\n",
    "                            yield chaches.pop(i)\n",
    "                        elif len(caches) == self.buffer_size:\n",
    "                            isfull = True\n",
    "                        while caches:\n",
    "                            i = np.random.randint(len(caches))\n",
    "                            yield caches.pop(i)\n",
    "            else:\n",
    "                def generator():\n",
    "                    indices = list(range(len(self.data)))\n",
    "                    np.random.shuffle(indices)\n",
    "                    for i in indices:\n",
    "                        yield self.data[i]\n",
    "            data = generator()\n",
    "        else:\n",
    "            data = iter(self.data)\n",
    "        \n",
    "        d_current = next(data) #该行代码保证最后返回的时候d_current有值\n",
    "        for d_next in data:\n",
    "            yield False, d_current\n",
    "            d_current = d_next\n",
    "        \n",
    "        yield True, d_current\n",
    "        \n",
    "    def __iter__(self, random=False):\n",
    "        batch_token_ids, batch_segment_ids, batch_labels = [], [], []\n",
    "        if self.train == True:\n",
    "            # 该处字段顺序与实际数据不一致导致训练报错\n",
    "            # for is_end, (label, ids, text) in self.sample(random):\n",
    "            for is_end, (ids, text, label) in self.sample(random):\n",
    "                batch_token_ids.append(word2idx_forward(text, self.vocab, self.MAX_SENT_LENGTH, self.MAX_SENTS))\n",
    "                batch_labels.append([label])\n",
    "                if len(batch_token_ids) == self.batch_size or is_end:\n",
    "                    batch_token_ids = sequence_padding(batch_token_ids)\n",
    "                    batch_labels = sequence_padding(batch_labels)\n",
    "                    yield batch_token_ids, batch_labels\n",
    "                    batch_token_ids, batch_segment_ids, batch_labels = [], [], []\n",
    "        else:\n",
    "            for is_end, (ids, text, label) in self.sample(random):\n",
    "                batch_token_ids.append(word2idx_forward(text, self.vocab, self.MAX_SENT_LENGTH, self.MAX_SENTS))\n",
    "                if len(batch_token_ids) == self.batch_size or is_end:\n",
    "                    batch_token_ids = sequence_padding(batch_token_ids)\n",
    "                    yield batch_token_ids\n",
    "                    batch_token_ids = []\n",
    "        \n",
    "    def forfit(self, random=True):\n",
    "        while True:\n",
    "            for d in self.__iter__(random):\n",
    "                yield d\n",
    "    \n",
    "    def forpredict(self, random=False):\n",
    "        while True:\n",
    "            for d in self.__iter__(random):\n",
    "                yield d\n",
    "\n",
    "class data_generator_backward(object):\n",
    "    \"\"\"数据生成器\n",
    "    \"\"\"\n",
    "    def __init__(self, data, batch_size,vocab, MAX_SENT_LENGTH, MAX_SENTS, train = True,buffer_size = None):\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "        self.train = train\n",
    "        self.vocab = vocab\n",
    "        self.MAX_SENT_LENGTH = MAX_SENT_LENGTH\n",
    "        self.MAX_SENTS = MAX_SENTS\n",
    "        if hasattr(self.data, '__len__'):\n",
    "            self.steps = len(self.data) // self.batch_size\n",
    "            if len(self.data) % self.batch_size != 0:\n",
    "                self.steps += 1\n",
    "        else:\n",
    "            self.steps = None\n",
    "        self.buffer_size = buffer_size or batch_size * 1000\n",
    "    def __len__(self):\n",
    "        return self.steps\n",
    "    \n",
    "    def sample(self, random=False):\n",
    "        if random:\n",
    "            if self.steps is None:\n",
    "                def generator():\n",
    "                    caches, isfull = [], False\n",
    "                    for d in self.data:\n",
    "                        caches.append(d)\n",
    "                        if isfull:\n",
    "                            i = np.random.randint(len(caches))\n",
    "                            yield chaches.pop(i)\n",
    "                        elif len(caches) == self.buffer_size:\n",
    "                            isfull = True\n",
    "                        while caches:\n",
    "                            i = np.random.randint(len(caches))\n",
    "                            yield caches.pop(i)\n",
    "            else:\n",
    "                def generator():\n",
    "                    indices = list(range(len(self.data)))\n",
    "                    np.random.shuffle(indices)\n",
    "                    for i in indices:\n",
    "                        yield self.data[i]\n",
    "            data = generator()\n",
    "        else:\n",
    "            data = iter(self.data)\n",
    "        \n",
    "        d_current = next(data)\n",
    "        for d_next in data:\n",
    "            yield False, d_current\n",
    "            d_current = d_next\n",
    "        \n",
    "        yield True, d_current\n",
    "        \n",
    "    def __iter__(self, random=False):\n",
    "        batch_token_ids, batch_segment_ids, batch_labels = [], [], []\n",
    "        if self.train == True:\n",
    "            for is_end, (ids, label,text) in self.sample(random):\n",
    "                batch_token_ids.append(word2idx_backward(text, self.vocab, self.MAX_SENT_LENGTH, self.MAX_SENTS))\n",
    "                batch_labels.append([label])\n",
    "                if len(batch_token_ids) == self.batch_size or is_end:\n",
    "                    batch_token_ids = sequence_padding(batch_token_ids)\n",
    "                    batch_labels = sequence_padding(batch_labels)\n",
    "                    yield batch_token_ids, batch_labels\n",
    "                    batch_token_ids, batch_segment_ids, batch_labels = [], [], []\n",
    "        else:\n",
    "            for is_end, (ids, text, label) in self.sample(random):\n",
    "                batch_token_ids.append(word2idx_backward(text, self.vocab, self.MAX_SENT_LENGTH, self.MAX_SENTS))\n",
    "                if len(batch_token_ids) == self.batch_size or is_end:\n",
    "                    batch_token_ids = sequence_padding(batch_token_ids)\n",
    "                    yield batch_token_ids\n",
    "                    batch_token_ids = []\n",
    "        \n",
    "    def forfit(self, random=True):\n",
    "        while True:\n",
    "            for d in self.__iter__(random):\n",
    "                yield d\n",
    "    \n",
    "    def forpredict(self, random=False):\n",
    "        while True:\n",
    "            for d in self.__iter__(random):\n",
    "                yield d\n",
    "def focal_loss(gamma=2., alpha=.25):\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "        return -K.mean(alpha * K.pow(1. - pt_1, gamma) * K.log(K.epsilon()+pt_1))-K.sum((1-alpha) * K.pow( pt_0, gamma) * K.log(1. - pt_0 + K.epsilon()))\n",
    "    return focal_loss_fixed\n",
    "\n",
    "# cut words\n",
    "def cut_text(sentence):\n",
    "    tokens = lac.run(sentence)\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: `shm_size_mb` is a deprecated argument. It will be removed in `pandarallel 2.0.0`.\n",
      "INFO: Pandarallel will run on 24 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n",
      "[['LAC', '是', '个', '优秀', '的', '分词', '工具'], ['nz', 'v', 'q', 'a', 'u', 'n', 'n']]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import gc\n",
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize(5)\n",
    "gc.collect()\n",
    "\n",
    "from LAC import LAC\n",
    "lac = LAC(mode='lac')\n",
    "lac.add_word('【科技实体】')\n",
    "text = u\"LAC是个优秀的分词工具\"\n",
    "lac_result = lac.run(text)\n",
    "print(lac_result)\n",
    "\n",
    "\n",
    "train = pd.read_csv('datasets/Train.csv')\n",
    "test = pd.read_csv('datasets/Test_B.csv')\n",
    "\n",
    "\n",
    "with open('lac/3000.txt','r' ) as f:\n",
    "    entitys = f.readlines()\n",
    "    entitys = [x.strip() for x in entitys]\n",
    "\n",
    "import json\n",
    "def entityResolution(line):\n",
    "    line = lac.run(line)\n",
    "    words,postag = line[0],line[1]\n",
    "    new_str = ''\n",
    "    for w in words:\n",
    "        if w in entitys:\n",
    "            new_str += '【科技实体】'\n",
    "        else:\n",
    "            new_str += w\n",
    "    return new_str\n",
    "\n",
    "\n",
    "train['Text_new'] = train['Text'].parallel_apply(lambda x: entityResolution(x[:512])) #和BERT不一样\n",
    "test['Text_new'] = test['Text'].parallel_apply(lambda x: entityResolution(x[:512]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reposition(data):\n",
    "    mid = data['text']\n",
    "    data.drop(labels=['text'], axis=1,inplace = True)\n",
    "    data.insert(1, 'text', mid)\n",
    "    return data\n",
    "train.drop(columns=['Domain','Abstract','Text'],inplace=True)\n",
    "train.rename(columns={'ID':'id','Text_new':'text','Label':'label'},inplace=True)\n",
    "train = reposition(train)\n",
    "\n",
    "test.drop(columns=['Text'],inplace=True)\n",
    "test.rename(columns={'ID':'id','Text_new':'text'},inplace=True)\n",
    "test = reposition(test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pretrainW2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lzy/anaconda3/envs/tensorflow/lib/python3.7/site-packages/scipy/sparse/sparsetools.py:21: DeprecationWarning: `scipy.sparse.sparsetools` is deprecated!\n",
      "scipy.sparse.sparsetools is a private module for scipy.sparse, and should not be used.\n",
      "  _deprecated()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69d41a8c52574a219cd50a3ab5f244cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='word count ing', max=46940.0, style=ProgressStyle(descrip…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aabc4ccd8abd4818a9f20d6d6a892466",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='filt ing...', max=321384.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.models.word2vec:collecting all words and their counts\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始训练词向量\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #10000, processed 2380890 words, keeping 12555 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #20000, processed 4725032 words, keeping 13001 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #30000, processed 6947796 words, keeping 13130 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #40000, processed 9165715 words, keeping 13130 word types\n",
      "INFO:gensim.models.word2vec:collected 13130 word types from a corpus of 10730866 raw words and 46940 sentences\n",
      "INFO:gensim.models.word2vec:Loading a fresh vocabulary\n",
      "INFO:gensim.models.word2vec:effective_min_count=50 retains 13130 unique words (100% of original 13130, drops 0)\n",
      "INFO:gensim.models.word2vec:effective_min_count=50 leaves 10730866 word corpus (100% of original 10730866, drops 0)\n",
      "INFO:gensim.models.word2vec:deleting the raw counts dictionary of 13130 items\n",
      "INFO:gensim.models.word2vec:sample=0.001 downsamples 26 most-common words\n",
      "INFO:gensim.models.word2vec:downsampling leaves estimated 8651136 word corpus (80.6% of prior 10730866)\n",
      "INFO:gensim.models.base_any2vec:estimated required memory for 13130 words and 100 dimensions: 17069000 bytes\n",
      "INFO:gensim.models.word2vec:resetting layer weights\n",
      "/home/lzy/anaconda3/envs/tensorflow/lib/python3.7/site-packages/ipykernel_launcher.py:52: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "INFO:gensim.models.base_any2vec:training model with 5 workers on 13130 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "INFO:gensim.models.base_any2vec:EPOCH 1 - PROGRESS: at 25.09% examples, 2249560 words/s, in_qsize 9, out_qsize 0\n",
      "INFO:gensim.models.base_any2vec:EPOCH 1 - PROGRESS: at 53.29% examples, 2343285 words/s, in_qsize 9, out_qsize 0\n",
      "INFO:gensim.models.base_any2vec:EPOCH 1 - PROGRESS: at 83.59% examples, 2413094 words/s, in_qsize 10, out_qsize 0\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.base_any2vec:EPOCH - 1 : training on 10730866 raw words (8652517 effective words) took 3.5s, 2454757 effective words/s\n",
      "INFO:gensim.models.base_any2vec:EPOCH 2 - PROGRESS: at 24.74% examples, 2216231 words/s, in_qsize 9, out_qsize 0\n",
      "INFO:gensim.models.base_any2vec:EPOCH 2 - PROGRESS: at 53.20% examples, 2339270 words/s, in_qsize 9, out_qsize 1\n",
      "INFO:gensim.models.base_any2vec:EPOCH 2 - PROGRESS: at 79.33% examples, 2290052 words/s, in_qsize 9, out_qsize 0\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.base_any2vec:EPOCH - 2 : training on 10730866 raw words (8651439 effective words) took 3.7s, 2327469 effective words/s\n",
      "INFO:gensim.models.base_any2vec:EPOCH 3 - PROGRESS: at 28.04% examples, 2507922 words/s, in_qsize 9, out_qsize 0\n",
      "INFO:gensim.models.base_any2vec:EPOCH 3 - PROGRESS: at 53.95% examples, 2369452 words/s, in_qsize 9, out_qsize 0\n",
      "INFO:gensim.models.base_any2vec:EPOCH 3 - PROGRESS: at 82.08% examples, 2368358 words/s, in_qsize 9, out_qsize 0\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.base_any2vec:EPOCH - 3 : training on 10730866 raw words (8651985 effective words) took 3.6s, 2371791 effective words/s\n",
      "INFO:gensim.models.base_any2vec:EPOCH 4 - PROGRESS: at 29.66% examples, 2644199 words/s, in_qsize 9, out_qsize 0\n",
      "INFO:gensim.models.base_any2vec:EPOCH 4 - PROGRESS: at 57.74% examples, 2523007 words/s, in_qsize 8, out_qsize 1\n",
      "INFO:gensim.models.base_any2vec:EPOCH 4 - PROGRESS: at 82.65% examples, 2377526 words/s, in_qsize 10, out_qsize 0\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.base_any2vec:EPOCH - 4 : training on 10730866 raw words (8651909 effective words) took 3.7s, 2354442 effective words/s\n",
      "INFO:gensim.models.base_any2vec:EPOCH 5 - PROGRESS: at 26.88% examples, 2402432 words/s, in_qsize 9, out_qsize 0\n",
      "INFO:gensim.models.base_any2vec:EPOCH 5 - PROGRESS: at 53.48% examples, 2345993 words/s, in_qsize 9, out_qsize 0\n",
      "INFO:gensim.models.base_any2vec:EPOCH 5 - PROGRESS: at 83.50% examples, 2404153 words/s, in_qsize 10, out_qsize 0\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.base_any2vec:EPOCH - 5 : training on 10730866 raw words (8651131 effective words) took 3.6s, 2399291 effective words/s\n",
      "INFO:gensim.models.base_any2vec:EPOCH 6 - PROGRESS: at 25.44% examples, 2279314 words/s, in_qsize 9, out_qsize 0\n",
      "INFO:gensim.models.base_any2vec:EPOCH 6 - PROGRESS: at 53.39% examples, 2345321 words/s, in_qsize 9, out_qsize 0\n",
      "INFO:gensim.models.base_any2vec:EPOCH 6 - PROGRESS: at 81.51% examples, 2347526 words/s, in_qsize 9, out_qsize 0\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.base_any2vec:EPOCH - 6 : training on 10730866 raw words (8650585 effective words) took 3.7s, 2356951 effective words/s\n",
      "INFO:gensim.models.base_any2vec:EPOCH 7 - PROGRESS: at 23.07% examples, 2062204 words/s, in_qsize 9, out_qsize 0\n",
      "INFO:gensim.models.base_any2vec:EPOCH 7 - PROGRESS: at 50.83% examples, 2237490 words/s, in_qsize 9, out_qsize 0\n",
      "INFO:gensim.models.base_any2vec:EPOCH 7 - PROGRESS: at 79.79% examples, 2305825 words/s, in_qsize 9, out_qsize 0\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.base_any2vec:EPOCH - 7 : training on 10730866 raw words (8649569 effective words) took 3.7s, 2321767 effective words/s\n",
      "INFO:gensim.models.base_any2vec:EPOCH 8 - PROGRESS: at 25.98% examples, 2323289 words/s, in_qsize 9, out_qsize 0\n",
      "INFO:gensim.models.base_any2vec:EPOCH 8 - PROGRESS: at 55.10% examples, 2416118 words/s, in_qsize 9, out_qsize 0\n",
      "INFO:gensim.models.base_any2vec:EPOCH 8 - PROGRESS: at 82.46% examples, 2378969 words/s, in_qsize 9, out_qsize 0\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.base_any2vec:EPOCH - 8 : training on 10730866 raw words (8651457 effective words) took 3.6s, 2370493 effective words/s\n",
      "INFO:gensim.models.base_any2vec:training on a 85846928 raw words (69210592 effective words) took 29.2s, 2367777 effective words/s\n",
      "INFO:gensim.utils:saving Word2Vec object under model_han_lin/w2v.model, separately None\n",
      "INFO:gensim.utils:not storing attribute vectors_norm\n",
      "INFO:gensim.utils:not storing attribute cum_table\n",
      "INFO:gensim.utils:saved model_han_lin/w2v.model\n",
      "INFO:gensim.models.utils_any2vec:storing 13130x100 projection weights into model_han_lin/word2vec_model.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import jieba\n",
    "import gensim\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "train = train\n",
    "test = test\n",
    "\n",
    "train_labels = train.label.values\n",
    "\n",
    "# with no stop_words\n",
    "stop_words = []\n",
    "\n",
    "vocab_set_path = 'model_han_lin/vocab_set.pkl'\n",
    "test['label'] = 0\n",
    "\n",
    "train['text'] = train['text'].parallel_apply(lambda x: cut_text(x)[0])\n",
    "test['text'] = test['text'].parallel_apply(lambda x: cut_text(x)[0])\n",
    "#词语计数\n",
    "\n",
    "word_cnt = collections.Counter()\n",
    "for line in tqdm(np.r_[train['text'].values,test['text'].values],desc= 'word count ing'):\n",
    "    word_cnt.update(line)\n",
    "min_count = 50\n",
    "word_cnt = [k for k in tqdm(word_cnt,desc='filt ing...') if word_cnt[k] > min_count] \n",
    "word_cnt = {k:v for v,k in enumerate(word_cnt)}\n",
    "\n",
    "vocab = word_cnt\n",
    "\n",
    "train['text'] = train['text'].parallel_apply(lambda x:[i for i in x if i in vocab])\n",
    "test['text'] = test['text'].parallel_apply(lambda x:[i for i in x if i in vocab])\n",
    "\n",
    "# save vocab\n",
    "save_variable(vocab,vocab_set_path) \n",
    "\n",
    "print('开始训练词向量')\n",
    "vector_size = 100 #2\n",
    "model = gensim.models.Word2Vec(size=vector_size, window=5, min_count=50, workers=5, sg=0, iter=8, seed=2021)\n",
    "model.build_vocab(np.r_[train['text'].values,test['text'].values])\n",
    "model.train(np.r_[train['text'].values,test['text'].values], total_examples=model.corpus_count, epochs=model.iter)\n",
    "model.save(\"model_han_lin/w2v.model\")\n",
    "word2vec_save = 'model_han_lin/word2vec_model.txt'\n",
    "model.wv.save_word2vec_format(word2vec_save, binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lzy/anaconda3/envs/tensorflow/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "from keras.models import Model\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from bert4keras.backend import keras, set_gelu\n",
    "from keras.layers import *\n",
    "from keras import backend as K\n",
    "from bert4keras.optimizers import Adam, extend_with_piecewise_linear_lr\n",
    "\n",
    "class AttentionLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__( **kwargs)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "        self.W = self.add_weight(name='att_weight',\n",
    "                                shape=(input_shape[1], input_shape[1]),\n",
    "                                initializer='uniform',\n",
    "                                trainable=True)\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "    \n",
    "    def call(self, inputs, mask=None):\n",
    "        x = K.permute_dimensions(inputs, (0, 2, 1))\n",
    "        a = K.softmax(K.tanh(K.dot(x, self.W)))\n",
    "        a = K.permute_dimensions(a, (0, 2, 1))\n",
    "        outputs = a * inputs\n",
    "        outputs = K.sum(outputs, axis=1)\n",
    "        return outputs\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], input_shape[2]\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "class SetLearningRate:\n",
    "    \"\"\"\n",
    "    \tlayer learning rate\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, layer, lamb, is_ada=False):\n",
    "        self.layer = layer\n",
    "        self.lamb = lamb # learning rate\n",
    "        self.is_ada = is_ada # if adam\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        with K.name_scope(self.layer.name):\n",
    "            if not self.layer.built:\n",
    "                input_shape = K.int_shape(inputs)\n",
    "                self.layer.build(input_shape)\n",
    "                self.layer.built = True\n",
    "                if self.layer._initial_weights is not None:\n",
    "                    self.layer.set_weights(self.layer._initial_weights)\n",
    "        for key in ['kernel', 'bias', 'embed', 'depthwise_kernel', 'pointwise_kernel', 'recurrent_kernel', 'gamma', 'beta']:\n",
    "            if hasattr(self.layer, key):\n",
    "                weight = getattr(self.layer, key)\n",
    "                if self.is_ada:\n",
    "                    lamb = self.lamb \n",
    "                else:\n",
    "                    lamb = self.lamb**0.5 # SGD（包括动量加速），lamb要开平方\n",
    "                K.set_value(weight, K.eval(weight) / lamb) # 更改初始化\n",
    "                setattr(self.layer, key, weight * lamb) # 按比例替换\n",
    "        return self.layer(inputs)\n",
    "\n",
    "\n",
    "class my_HAN_model(object):\n",
    "    def __init__(self, MAX_SENT_LENGTH, MAX_SENTS, embedding_matrix, vocab):\n",
    "    \tself.MAX_SENT_LENGTH = MAX_SENT_LENGTH\n",
    "    \tself.MAX_SENTS = MAX_SENTS\n",
    "    \tself.embedding_matrix = embedding_matrix\n",
    "    \tself.vocab = vocab\n",
    "    def create_model(self):\n",
    "        sentence_inputs = Input(shape=(self.MAX_SENT_LENGTH,), dtype='float64')\n",
    "        \n",
    "        embed = Embedding(len(self.vocab) + 1, 100, input_length=self.MAX_SENT_LENGTH, weights=[self.embedding_matrix], trainable=True)\n",
    "        embed = SetLearningRate(embed, 0.001, True)(sentence_inputs)\n",
    "        l_lstm = Bidirectional(GRU(128, return_sequences=True))(embed)\n",
    "        l_dense = TimeDistributed(Dense(64))(l_lstm)\n",
    "        print(l_dense.shape)\n",
    "        l_att = AttentionLayer()(l_dense)\n",
    "        print(l_att.shape)\n",
    "        sentEncoder = Model(sentence_inputs, l_att)\n",
    "        \n",
    "        review_input = Input(shape=(self.MAX_SENTS, self.MAX_SENT_LENGTH), dtype='int32')\n",
    "        review_encoder = TimeDistributed(sentEncoder)(review_input)\n",
    "        l_lstm_sent = Bidirectional(GRU(128, return_sequences=True))(review_encoder)\n",
    "        l_dense_sent = TimeDistributed(Dense(64))(l_lstm_sent)\n",
    "        l_att_sent = AttentionLayer()(l_dense_sent)\n",
    "        outputs = Dense(1, activation='sigmoid')(l_att_sent)\n",
    "        \n",
    "        self.model = Model(review_input, outputs=outputs)\n",
    "        self.compile()\n",
    "        return self.model\n",
    "    \n",
    "    def compile(self):\n",
    "        self.model.compile(\n",
    "             loss='binary_crossentropy',\n",
    "            optimizer=Adam(1e-3),  \n",
    "            metrics=['accuracy'],\n",
    "        )\n",
    "\n",
    "epsilon = 1e-5\n",
    "smooth = 1\n",
    "def tversky(y_true, y_pred):\n",
    "    y_true_pos = K.flatten(y_true)\n",
    "    y_pred_pos = K.flatten(y_pred)\n",
    "    true_pos = K.sum(y_true_pos * y_pred_pos)\n",
    "    false_neg = K.sum(y_true_pos * (1-y_pred_pos))\n",
    "    false_pos = K.sum((1-y_true_pos)*y_pred_pos)\n",
    "    alpha = 0.7\n",
    "    return (true_pos + smooth)/(true_pos + alpha*false_neg + (1-alpha)*false_pos + smooth)\n",
    "\n",
    "def tversky_loss(y_true, y_pred):\n",
    "    return 1 - tversky(y_true, y_pred)\n",
    "\n",
    "def focal_tversky(y_true,y_pred):\n",
    "    pt_1 = tversky(y_true, y_pred)\n",
    "    gamma = 0.75\n",
    "    return K.pow((1-pt_1), gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 4 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize(nb_workers = 4)\n",
    "def mysplit_keep_puntuation(s, punctuation='!~。？?,，；'):\n",
    "    '''\n",
    "    :param s: input string\n",
    "    :param punctuation: 需要分割的标点\n",
    "    :return: 切分结果，保留标点\n",
    "    '''\n",
    "    res_list = []\n",
    "    buff = ''\n",
    "\n",
    "    for c in s:\n",
    "        buff += c\n",
    "        if c in punctuation:\n",
    "            res_list.append(buff)\n",
    "            buff = ''\n",
    "    if buff != '':\n",
    "        res_list.append(buff)\n",
    "    return res_list\n",
    "def reposition(data):\n",
    "    mid = data['text']\n",
    "    data.drop(labels=['text'], axis=1,inplace = True)\n",
    "    data.insert(1, 'text', mid)\n",
    "    return data\n",
    "def dataProcess(data,vocab):\n",
    "    data['text'] = data['text'].parallel_apply(lambda x: cut_text(x)[0])\n",
    "    data['text'] = data['text'].parallel_apply(lambda x:''.join([i for i in x if i in vocab]))\n",
    "    data['text'] = data['text'].parallel_apply(lambda x:mysplit_keep_puntuation(x,'。'))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.utils:loading Word2Vec object from model_han_lin/w2v.model\n",
      "INFO:gensim.utils:loading wv recursively from model_han_lin/w2v.model.wv.* with mmap=None\n",
      "INFO:gensim.utils:setting ignored attribute vectors_norm to None\n",
      "INFO:gensim.utils:loading vocabulary recursively from model_han_lin/w2v.model.vocabulary.* with mmap=None\n",
      "INFO:gensim.utils:loading trainables recursively from model_han_lin/w2v.model.trainables.* with mmap=None\n",
      "INFO:gensim.utils:setting ignored attribute cum_table to None\n",
      "INFO:gensim.utils:loaded model_han_lin/w2v.model\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Model\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from bert4keras.backend import keras, set_gelu\n",
    "from keras.layers import *\n",
    "from keras import backend as K\n",
    "from sklearn import metrics\n",
    "import jieba\n",
    "import gensim\n",
    "import os\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import Sequence\n",
    "from bert4keras.optimizers import Adam, extend_with_piecewise_linear_lr\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "# from utils import save_variable, load_variavle, data_generator_forward, data_generator_backward, Evaluator\n",
    "# from model import my_HAN_model\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "resample_flag = True\n",
    "MAX_SENT_LENGTH = 100\n",
    "MAX_SENTS = 10\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "#读取字典、模型\n",
    "vocab = load_variavle('model_han_lin/vocab_set.pkl')\n",
    "w2v_model = gensim.models.Word2Vec.load(\"model_han_lin/w2v.model\")\n",
    "\n",
    "train = pd.read_csv(\"datasets/Train.csv\")\n",
    "train['Text_new'] = train['Text'].parallel_apply(lambda x: entityResolution(x[:512]))\n",
    "train.drop(columns=['Domain','Abstract','Text'],inplace=True)\n",
    "train.rename(columns={'ID':'id','Text_new':'text','Label':'label'},inplace=True)\n",
    "train = reposition(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt = 0\n",
    "for n,line in train_df.iterrows():\n",
    "    text = line['text']\n",
    "    for i in text[:10]:\n",
    "        if '【科技实体】' in i[:100] line['label'] == 1:\n",
    "#             print(n)\n",
    "            cnt += 1\n",
    "            break\n",
    "cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data,model):\n",
    "    total, right = 0., 0.\n",
    "    y_trues = []\n",
    "    y_preds = []\n",
    "    for x_true, y_true in data:\n",
    "        y_preds = np.r_[y_preds, model.predict(x_true)[:,0]]\n",
    "        y_trues = np.r_[y_trues, y_true[:, 0]]\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_trues, y_preds)\n",
    "    return  metrics.auc(fpr, tpr)\n",
    "\n",
    "def evaluate_recall(data,model):\n",
    "    total, right = 0., 0.\n",
    "    y_trues = []\n",
    "    y_preds = []\n",
    "    for x_true, y_true in data:\n",
    "        y_preds = np.r_[y_preds, model.predict(x_true)[:,0]]\n",
    "        y_trues = np.r_[y_trues, y_true[:, 0]]\n",
    "    fscore = metrics.f1_score(y_trues, y_preds > 0.5, pos_label=1, average='binary')\n",
    "    \n",
    "    print(fscore)\n",
    "    thresholds = list(np.arange(0.0, 1.0, 0.001))\n",
    "    acc_scores = np.zeros(shape=(len(thresholds)))\n",
    "    for index, elem in tqdm(enumerate(thresholds),total=len(thresholds),disable = True ):\n",
    "        # 修正概率\n",
    "        y_pred_prob = (y_preds > elem).astype('int')\n",
    "        # 计算f值\n",
    "        acc_scores[index] =  metrics.f1_score(y_trues,y_pred_prob,pos_label=1,average='binary')\n",
    "    index = np.argmax(acc_scores)\n",
    "    thresholdOpt = round(thresholds[index], ndigits = 5)\n",
    "    best = round(acc_scores[index], ndigits = 5)\n",
    "    return  best,thresholdOpt\n",
    "class Evaluator(keras.callbacks.Callback):\n",
    "    \"\"\"评估与保存\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name,valid_generator,model):\n",
    "        self.best_val_acc = 0\n",
    "        self.model_name = model_name\n",
    "        self.valid_generator = valid_generator\n",
    "        self.model = model\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        val_acc = evaluate(self.valid_generator, self.model) #auc\n",
    "        best,threshold = evaluate_recall(self.valid_generator, self.model)\n",
    "        # if (val_acc > self.best_val_acc):\n",
    "        if (best > self.best_val_acc):\n",
    "            print('1')\n",
    "            self.best_val_acc = best\n",
    "            self.best_threshold = threshold\n",
    "            self.model.save_weights(self.model_name)\n",
    "        print(\n",
    "            u'val_auc: %.5f, best_val_auc: %.5f, val_f1: %.5f' %\n",
    "            (val_acc, self.best_val_acc, best)\n",
    "        )\n",
    "        logs['val_auc'] = val_acc\n",
    "        logs['val_recall'] = best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lzy/anaconda3/envs/tensorflow/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "/home/lzy/anaconda3/envs/tensorflow/lib/python3.7/site-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"\n",
      "/home/lzy/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/framework/tensor_util.py:523: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  tensor_proto.tensor_content = nparray.tostring()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 100, 64)\n",
      "(None, 64)\n",
      "开始训练\n",
      "Epoch 1/2\n",
      "339/339 [==============================] - 9s 27ms/step - loss: 0.2877 - accuracy: 0.8871\n",
      "Epoch 2/2\n",
      "339/339 [==============================] - 9s 26ms/step - loss: 0.1376 - accuracy: 0.9517\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 预训练的词向量中没有出现的词用0向量表示\n",
    "embedding_matrix = np.zeros((len(vocab) + 1, 100))\n",
    "for word, i in vocab.items():\n",
    "    try:\n",
    "        embedding_vector = w2v_model[str(word)]\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    except KeyError:\n",
    "        continue\n",
    "\n",
    "# 5折\n",
    "train_entity = train[train['text'].str.contains('【科技实体】')]\n",
    "train_pos = train[train['label']==1]\n",
    "train_neg = train[train['label']==0].sample(2000,random_state=123)\n",
    "train_df = pd.concat([train_entity,train_pos,train_neg]).drop_duplicates()\n",
    "train_df = dataProcess(train_df,vocab)\n",
    "\n",
    "\n",
    "train_batch_size = 8 #16\n",
    "\n",
    "# kold = StratifiedKFold(random_state=2020,shuffle=True,   n_splits=5).split(train_df, train_df['label'])\n",
    "# for fold, (train_idx, valid_idx) in enumerate(kold):\n",
    "K.clear_session()\n",
    "md = my_HAN_model(MAX_SENT_LENGTH = MAX_SENT_LENGTH, MAX_SENTS=MAX_SENTS, embedding_matrix=embedding_matrix,vocab=vocab)\n",
    "model = md.create_model()\n",
    "\n",
    "train_generator = data_generator_forward(train_df.values, train_batch_size, vocab, MAX_SENT_LENGTH, MAX_SENTS) \n",
    "#valid_generator = data_generator_forward(train_df.values[valid_idx], train_batch_size, vocab, MAX_SENT_LENGTH, MAX_SENTS)\n",
    "\n",
    "print(\"开始训练\")\n",
    "#evaluator = Evaluator(model_name='model_han_lin/best_model{}.weights'.format(fold), valid_generator=valid_generator, model=model)\n",
    "#earlystop = EarlyStopping(monitor='val_recall',patience=5, mode='max')\n",
    "#lr_reduce = ReduceLROnPlateau(monitor='val_recall', patience=2, verbose=1, factor=0.1, min_lr=1e-5, mode='max')\n",
    "\n",
    "model.fit_generator(\n",
    "    train_generator.forfit(), \n",
    "    steps_per_epoch=len(train_generator),\n",
    "    epochs= 2,\n",
    "    #callbacks=[evaluator, lr_reduce, earlystop]\n",
    "\n",
    ")\n",
    "model.save_weights('model_han_lin/best_model{}.weights'.format(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lzy/anaconda3/envs/tensorflow/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SessionId</th>\n",
       "      <th>text</th>\n",
       "      <th>HighRiskFlag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[意见领袖专栏作家“保险科技”概念在2017年被正式提出之后，“互联网保险”逐渐成为保险科技...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[报道3月23日，国内【科技实体】（，以下称众安）发布2020年业绩报告，公司人民币，实现扭...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[3月23日，国内【科技实体】(，以下称众安)发布2020年业绩报告，公司人民币，实现扭亏为...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[此前，水滴公司宣布未来三年将投入20亿元搭建保险科技新基建。, 水滴公司基于对大健康行业的...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[2020年7月2日，一家名为（，【科技实体】）的公司在纽交所上市，当日以开盘之后，盘中股价...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1495</th>\n",
       "      <td>1495</td>\n",
       "      <td>[本文来源于前瞻IPO微信公众上会新股过会披露了.(百融云创)的招股书及聆讯后。, 、、为其...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1496</th>\n",
       "      <td>1496</td>\n",
       "      <td>[未经授权，本文严禁转载，否则追究法律责任。, ：——有害信息：——，与签订了战略合作框架协...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1497</th>\n",
       "      <td>1497</td>\n",
       "      <td>[近日，（以下简称“中国平安”）公布截至2021年3月31日止三个月期间业绩显示，2021年...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1498</th>\n",
       "      <td>1498</td>\n",
       "      <td>[图为：（记者）在5月25日下午召开的武汉区域金融中心建设论坛上，《武汉市打造区域金融中心实...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1499</th>\n",
       "      <td>1499</td>\n",
       "      <td>[水滴公司创始人沈鹏曾表示，水滴公司本质上希望能够成为我国医保之外的一个商业保障体系，以普惠...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1500 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      SessionId                                               text  \\\n",
       "0             0  [意见领袖专栏作家“保险科技”概念在2017年被正式提出之后，“互联网保险”逐渐成为保险科技...   \n",
       "1             1  [报道3月23日，国内【科技实体】（，以下称众安）发布2020年业绩报告，公司人民币，实现扭...   \n",
       "2             2  [3月23日，国内【科技实体】(，以下称众安)发布2020年业绩报告，公司人民币，实现扭亏为...   \n",
       "3             3  [此前，水滴公司宣布未来三年将投入20亿元搭建保险科技新基建。, 水滴公司基于对大健康行业的...   \n",
       "4             4  [2020年7月2日，一家名为（，【科技实体】）的公司在纽交所上市，当日以开盘之后，盘中股价...   \n",
       "...         ...                                                ...   \n",
       "1495       1495  [本文来源于前瞻IPO微信公众上会新股过会披露了.(百融云创)的招股书及聆讯后。, 、、为其...   \n",
       "1496       1496  [未经授权，本文严禁转载，否则追究法律责任。, ：——有害信息：——，与签订了战略合作框架协...   \n",
       "1497       1497  [近日，（以下简称“中国平安”）公布截至2021年3月31日止三个月期间业绩显示，2021年...   \n",
       "1498       1498  [图为：（记者）在5月25日下午召开的武汉区域金融中心建设论坛上，《武汉市打造区域金融中心实...   \n",
       "1499       1499  [水滴公司创始人沈鹏曾表示，水滴公司本质上希望能够成为我国医保之外的一个商业保障体系，以普惠...   \n",
       "\n",
       "      HighRiskFlag  \n",
       "0                1  \n",
       "1                1  \n",
       "2                1  \n",
       "3                1  \n",
       "4                1  \n",
       "...            ...  \n",
       "1495             0  \n",
       "1496             0  \n",
       "1497             0  \n",
       "1498             0  \n",
       "1499             0  \n",
       "\n",
       "[1500 rows x 3 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('datasets/Test_B.csv')\n",
    "test['Text_new'] = test['Text'].parallel_apply(lambda x: entityResolution(x[:512]))\n",
    "test.drop(columns =['Text'],inplace=True)\n",
    "test.rename(columns={'ID':'id','Text_new':'text'},inplace=True)\n",
    "test = reposition(test)\n",
    "test = dataProcess(test,vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 100, 64)\n",
      "(None, 64)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "\tUse thie file to generate result.\n",
    "'''\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "from keras.models import Model\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from bert4keras.backend import keras, set_gelu\n",
    "from keras.layers import *\n",
    "from keras import backend as K\n",
    "from sklearn import metrics\n",
    "import gensim\n",
    "import os\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import Sequence\n",
    "from bert4keras.optimizers import Adam, extend_with_piecewise_linear_lr\n",
    "\n",
    "batch_size = 32\n",
    "test['Label'] = 0\n",
    "\n",
    "\n",
    "K.clear_session()\n",
    "md = my_HAN_model(MAX_SENT_LENGTH = MAX_SENT_LENGTH, MAX_SENTS=MAX_SENTS, embedding_matrix=embedding_matrix,vocab=vocab)\n",
    "model = md.create_model()\n",
    "test_generator = data_generator_forward(test.values, batch_size, vocab, MAX_SENT_LENGTH, MAX_SENTS, train=False)\n",
    "model.load_weights(r'model_han_lin/best_model{}.weights'.format(1))\n",
    "probs = model.predict_generator(test_generator.forpredict(),\n",
    "    steps=len(test_generator))[:,0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.array(probs)\n",
    "r = []\n",
    "for pred in y_pred:\n",
    "    pred = np.where(pred > 0.5,1,0)\n",
    "    r.append(pred)\n",
    "r = np.array(r)\n",
    "vote = r.sum(axis=0)\n",
    "vote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('datasets/Test_B.csv')\n",
    "submit_data = test[['id']]\n",
    "submit_data[\"Label\"]=r\n",
    "submit_data.to_csv(\"result/result_han_0.68.csv\",index=False)\n",
    "display(submit_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "152"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true = [1] * 212 + [0] * 1288\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "f1_score(true,r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6813186813186812"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SessionId</th>\n",
       "      <th>text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[意见领袖专栏作家“保险科技”概念在2017年被正式提出之后，“互联网保险”逐渐成为保险科技...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[报道3月23日，国内【科技实体】（，以下称众安）发布2020年业绩报告，公司人民币，实现扭...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[3月23日，国内【科技实体】(，以下称众安)发布2020年业绩报告，公司人民币，实现扭亏为...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[此前，水滴公司宣布未来三年将投入20亿元搭建保险科技新基建。, 水滴公司基于对大健康行业的...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[2020年7月2日，一家名为（，【科技实体】）的公司在纽交所上市，当日以开盘之后，盘中股价...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1495</th>\n",
       "      <td>1495</td>\n",
       "      <td>[本文来源于前瞻IPO微信公众上会新股过会披露了.(百融云创)的招股书及聆讯后。, 、、为其...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1496</th>\n",
       "      <td>1496</td>\n",
       "      <td>[未经授权，本文严禁转载，否则追究法律责任。, ：——有害信息：——，与签订了战略合作框架协...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1497</th>\n",
       "      <td>1497</td>\n",
       "      <td>[近日，（以下简称“中国平安”）公布截至2021年3月31日止三个月期间业绩显示，2021年...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1498</th>\n",
       "      <td>1498</td>\n",
       "      <td>[图为：（记者）在5月25日下午召开的武汉区域金融中心建设论坛上，《武汉市打造区域金融中心实...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1499</th>\n",
       "      <td>1499</td>\n",
       "      <td>[水滴公司创始人沈鹏曾表示，水滴公司本质上希望能够成为我国医保之外的一个商业保障体系，以普惠...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1500 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      SessionId                                               text  Label\n",
       "0             0  [意见领袖专栏作家“保险科技”概念在2017年被正式提出之后，“互联网保险”逐渐成为保险科技...      0\n",
       "1             1  [报道3月23日，国内【科技实体】（，以下称众安）发布2020年业绩报告，公司人民币，实现扭...      0\n",
       "2             2  [3月23日，国内【科技实体】(，以下称众安)发布2020年业绩报告，公司人民币，实现扭亏为...      0\n",
       "3             3  [此前，水滴公司宣布未来三年将投入20亿元搭建保险科技新基建。, 水滴公司基于对大健康行业的...      0\n",
       "4             4  [2020年7月2日，一家名为（，【科技实体】）的公司在纽交所上市，当日以开盘之后，盘中股价...      0\n",
       "...         ...                                                ...    ...\n",
       "1495       1495  [本文来源于前瞻IPO微信公众上会新股过会披露了.(百融云创)的招股书及聆讯后。, 、、为其...      0\n",
       "1496       1496  [未经授权，本文严禁转载，否则追究法律责任。, ：——有害信息：——，与签订了战略合作框架协...      0\n",
       "1497       1497  [近日，（以下简称“中国平安”）公布截至2021年3月31日止三个月期间业绩显示，2021年...      0\n",
       "1498       1498  [图为：（记者）在5月25日下午召开的武汉区域金融中心建设论坛上，《武汉市打造区域金融中心实...      0\n",
       "1499       1499  [水滴公司创始人沈鹏曾表示，水滴公司本质上希望能够成为我国医保之外的一个商业保障体系，以普惠...      0\n",
       "\n",
       "[1500 rows x 3 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lzy/anaconda3/envs/tensorflow/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "/home/lzy/anaconda3/envs/tensorflow/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SessionId</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1495</th>\n",
       "      <td>1495</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1496</th>\n",
       "      <td>1496</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1497</th>\n",
       "      <td>1497</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1498</th>\n",
       "      <td>1498</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1499</th>\n",
       "      <td>1499</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1500 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      SessionId  Label\n",
       "0             0      0\n",
       "1             1      0\n",
       "2             2      0\n",
       "3             3      0\n",
       "4             4      1\n",
       "...         ...    ...\n",
       "1495       1495      0\n",
       "1496       1496      0\n",
       "1497       1497      0\n",
       "1498       1498      0\n",
       "1499       1499      0\n",
       "\n",
       "[1500 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lzy/anaconda3/envs/tensorflow/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SessionId    1101672\n",
       "Label             61\n",
       "dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lzy/anaconda3/envs/tensorflow/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SessionId</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1441</th>\n",
       "      <td>1441</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1453</th>\n",
       "      <td>1453</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>1457</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1478</th>\n",
       "      <td>1478</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1484</th>\n",
       "      <td>1484</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>182 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      SessionId  Label\n",
       "4             4      1\n",
       "5             5      1\n",
       "6             6      1\n",
       "7             7      1\n",
       "8             8      1\n",
       "...         ...    ...\n",
       "1441       1441      1\n",
       "1453       1453      1\n",
       "1457       1457      1\n",
       "1478       1478      1\n",
       "1484       1484      1\n",
       "\n",
       "[182 rows x 2 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lzy/anaconda3/envs/tensorflow/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'toLabel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-bcc1f095db74>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mvotes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mth\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mvotes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoLabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mvotes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvotes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>=\u001b[0m\u001b[0;36m0.6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mvotes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'toLabel' is not defined"
     ]
    }
   ],
   "source": [
    "votes = []\n",
    "for pred,th in zip(probs,[0.5,0.5,0.5,0.5,0.5]):\n",
    "    votes.append(toLabel(pred,th))\n",
    "votes = np.where(np.mean(votes,axis=0)>=0.6,1,0)\n",
    "votes.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best = pd.read_csv('result/han5_1000_0057_278.csv')\n",
    "best['me'] = votes\n",
    "best[(best.Label!=best.me)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toLabel(pred,th):\n",
    "    res = np.where(pred>th,1,0)\n",
    "    print(res.sum())\n",
    "    return res\n",
    "sub = test.copy()\n",
    "sub['ID'] = sub['id']\n",
    "sub['Label'] = votes\n",
    "sub['Abstract'] = ''\n",
    "sub[['ID','Label','Abstract']].to_excel('result/lx_classify1_sample0.6_384.xls',index=None,encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
